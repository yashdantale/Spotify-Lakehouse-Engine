{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY-q0Xfi3p0h"
      },
      "outputs": [],
      "source": [
        "# --- CELL 1: DUAL INGESTION (REAL + SYNTHETIC) ---\n",
        "!pip install spotipy faker --quiet\n",
        "\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import random\n",
        "import shutil\n",
        "from google.colab import drive, userdata\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyOAuth\n",
        "\n",
        "# 1. Setup Paths & Cleanup\n",
        "drive.mount('/content/drive')\n",
        "base_path = \"/content/drive/MyDrive/Spotify_Lakehouse\"\n",
        "bronze_path = f\"{base_path}/bronze\"\n",
        "silver_path = f\"{base_path}/silver\"\n",
        "\n",
        "# Clear Bronze to prevent duplicates\n",
        "if os.path.exists(bronze_path):\n",
        "    shutil.rmtree(bronze_path)\n",
        "os.makedirs(bronze_path)\n",
        "os.makedirs(silver_path, exist_ok=True)\n",
        "\n",
        "# 2. Auth\n",
        "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(\n",
        "    client_id=userdata.get('SPOTIFY_CLIENT_ID'),\n",
        "    client_secret=userdata.get('SPOTIFY_CLIENT_SECRET'),\n",
        "    redirect_uri=\"http://127.0.0.1:8888/callback\",\n",
        "    scope=\"user-read-recently-played\",\n",
        "    open_browser=False\n",
        "))\n",
        "\n",
        "# --- GENERATOR 1: SYNTHETIC DATA ---\n",
        "print(\"Generating Synthetic Data...\")\n",
        "mock_playlist = [\n",
        "    (\"The Weeknd\", \"Blinding Lights\", \"After Hours\"),\n",
        "    (\"Taylor Swift\", \"Cruel Summer\", \"Lover\"),\n",
        "    (\"Arijit Singh\", \"Kesariya\", \"Brahmastra\"),\n",
        "    (\"Drake\", \"God's Plan\", \"Scorpion\"),\n",
        "    (\"Queen\", \"Bohemian Rhapsody\", \"A Night at the Opera\"),\n",
        "    (\"Coldplay\", \"Yellow\", \"Parachutes\")\n",
        "]\n",
        "fake_items = []\n",
        "current_time = datetime.datetime.now()\n",
        "for _ in range(50):\n",
        "    artist, track, album = random.choice(mock_playlist)\n",
        "    minutes_ago = random.randint(0, 1440)\n",
        "    played_at = (current_time - datetime.timedelta(minutes=minutes_ago)).isoformat() + \"Z\"\n",
        "    fake_items.append({\n",
        "        \"played_at\": played_at,\n",
        "        \"track\": {\n",
        "            \"name\": track, \"duration_ms\": 200000, \"popularity\": 80,\n",
        "            \"album\": {\"name\": album}, \"artists\": [{\"name\": artist}]\n",
        "        }\n",
        "    })\n",
        "\n",
        "# Save Synthetic File\n",
        "with open(f\"{bronze_path}/synthetic.json\", 'w') as f:\n",
        "    json.dump({\"items\": fake_items}, f)\n",
        "print(\"Synthetic Data Saved.\")\n",
        "\n",
        "# --- GENERATOR 2: REAL DATA ---\n",
        "print(\"Fetching Real Data...\")\n",
        "try:\n",
        "    results = sp.current_user_recently_played(limit=50)\n",
        "    if len(results['items']) > 0:\n",
        "        with open(f\"{bronze_path}/real.json\", 'w') as f:\n",
        "            json.dump(results, f)\n",
        "        print(f\"Real Data Saved ({len(results['items'])} tracks).\")\n",
        "    else:\n",
        "        print(\"Real Data is empty (Spotify API returned 0). Only Synthetic available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Real Data Error: {e}\")\n",
        "\n",
        "print(\"Ingestion Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 2: PYSPARK ETL (TAGGING SOURCES) ---\n",
        "!pip install pyspark -q\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, from_utc_timestamp, hour, when, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SpotifyLakehouse\").getOrCreate()\n",
        "\n",
        "# 1. Define Schema\n",
        "track_schema = StructType([\n",
        "    StructField(\"items\", ArrayType(\n",
        "        StructType([\n",
        "            StructField(\"played_at\", StringType(), True),\n",
        "            StructField(\"track\", StructType([\n",
        "                StructField(\"name\", StringType(), True),\n",
        "                StructField(\"artists\", ArrayType(StructType([StructField(\"name\", StringType(), True)])), True),\n",
        "                StructField(\"album\", StructType([StructField(\"name\", StringType(), True)]), True)\n",
        "            ]), True)\n",
        "        ])\n",
        "    ), True)\n",
        "])\n",
        "\n",
        "# 2. Function to Process a specific file and add a Tag\n",
        "def process_file(filename, source_tag):\n",
        "    full_path = f\"{bronze_path}/{filename}\"\n",
        "    if not os.path.exists(full_path):\n",
        "        return None\n",
        "\n",
        "    raw = spark.read.schema(track_schema).option(\"multiline\", \"true\").json(full_path)\n",
        "\n",
        "    return raw.select(explode(\"items\").alias(\"item\")) \\\n",
        "        .select(\n",
        "            col(\"item.track.name\").alias(\"track_name\"),\n",
        "            col(\"item.track.artists\")[0][\"name\"].alias(\"artist_name\"),\n",
        "            col(\"item.played_at\").alias(\"played_at_utc\")\n",
        "        ) \\\n",
        "        .withColumn(\"played_at_ist\", from_utc_timestamp(col(\"played_at_utc\"), \"IST\")) \\\n",
        "        .withColumn(\"hour\", hour(col(\"played_at_ist\"))) \\\n",
        "        .withColumn(\"time_period\",\n",
        "            when((col(\"hour\") >= 5) & (col(\"hour\") < 12), \"Morning ðŸŒ…\")\n",
        "            .when((col(\"hour\") >= 12) & (col(\"hour\") < 17), \"Afternoon â˜€ï¸\")\n",
        "            .when((col(\"hour\") >= 17) & (col(\"hour\") < 21), \"Evening ðŸŒ†\")\n",
        "            .otherwise(\"Night ðŸŒ™\")\n",
        "        ) \\\n",
        "        .withColumn(\"data_source\", lit(source_tag)) # <--- THIS IS THE MAGIC TAG\n",
        "\n",
        "# 3. Process Both\n",
        "df_syn = process_file(\"synthetic.json\", \"Synthetic Data\")\n",
        "df_real = process_file(\"real.json\", \"Real Spotify Data\")\n",
        "\n",
        "# 4. Combine (Union)\n",
        "if df_real is not None:\n",
        "    final_df = df_syn.union(df_real)\n",
        "else:\n",
        "    final_df = df_syn\n",
        "\n",
        "# 5. Write to Silver\n",
        "final_df.write.mode(\"overwrite\").parquet(f\"{silver_path}/tracks.parquet\")\n",
        "print(\"ETL Success: Combined Data saved.\")\n",
        "final_df.groupBy(\"data_source\").count().show()"
      ],
      "metadata": {
        "id": "e0ps3IVz4dWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 3: APP WITH TOGGLE (NO CACHE) ---\n",
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import glob\n",
        "\n",
        "st.set_page_config(page_title=\"Spotify Lakehouse\", layout=\"wide\")\n",
        "\n",
        "# --- SIDEBAR CONTROLS ---\n",
        "st.sidebar.title(\"ðŸŽ›ï¸ Controls\")\n",
        "st.sidebar.markdown(\"Use this switch to compare Real vs. Simulated data.\")\n",
        "\n",
        "# 1. Load Data (CACHE REMOVED)\n",
        "# We removed @st.cache_data so it re-reads the file every time you refresh.\n",
        "def load_data():\n",
        "    path = \"/content/drive/MyDrive/Spotify_Lakehouse/silver/tracks.parquet\"\n",
        "    if not glob.glob(f\"{path}/*.parquet\"):\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_parquet(path, engine='pyarrow')\n",
        "\n",
        "df_all = load_data()\n",
        "\n",
        "if not df_all.empty:\n",
        "    # 2. THE TOGGLE SWITCH\n",
        "    available_sources = df_all['data_source'].unique()\n",
        "\n",
        "    # Default to 'Real' if available\n",
        "    index = 0\n",
        "    if \"Real Spotify Data\" in available_sources:\n",
        "        index = list(available_sources).index(\"Real Spotify Data\")\n",
        "\n",
        "    source_selection = st.sidebar.radio(\n",
        "        \"Select Data Source:\",\n",
        "        available_sources,\n",
        "        index=index\n",
        "    )\n",
        "\n",
        "    # 3. FILTER DATA\n",
        "    df = df_all[df_all['data_source'] == source_selection]\n",
        "\n",
        "    # --- DASHBOARD UI ---\n",
        "    st.title(f\"ðŸŽ§ Dashboard: {source_selection}\")\n",
        "\n",
        "    # Metrics\n",
        "    c1, c2, c3 = st.columns(3)\n",
        "    c1.metric(\"Tracks Analyzed\", len(df))\n",
        "    c2.metric(\"Unique Artists\", df['artist_name'].nunique())\n",
        "    top_artist = df['artist_name'].mode()[0] if not df.empty else \"N/A\"\n",
        "    c3.metric(\"Top Artist\", top_artist)\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    # Charts\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"ðŸŽ¤ Top Artists\")\n",
        "        if not df.empty:\n",
        "            counts = df['artist_name'].value_counts().head(10).reset_index()\n",
        "            counts.columns = ['Artist', 'Count']\n",
        "            fig = px.bar(counts, x='Count', y='Artist', orientation='h', color='Count')\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"ðŸ•’ Listening Time\")\n",
        "        if not df.empty:\n",
        "            t_counts = df['time_period'].value_counts().reset_index()\n",
        "            t_counts.columns = ['Period', 'Count']\n",
        "            fig2 = px.pie(t_counts, values='Count', names='Period', hole=0.4)\n",
        "            st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    st.subheader(\"ðŸ“„ Raw Data\")\n",
        "    st.dataframe(df.head(10), use_container_width=True)\n",
        "\n",
        "else:\n",
        "    st.error(\"No data found. Run ETL first.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "print(\"App updated (Cache Disabled).\")"
      ],
      "metadata": {
        "id": "ESYH3Es657jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 4: LAUNCH ---\n",
        "!pip install streamlit pyngrok plotly pandas pyarrow -q\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Auth\n",
        "try:\n",
        "    ngrok.set_auth_token(userdata.get('NGROK_AUTH_TOKEN'))\n",
        "except:\n",
        "    print(\"Check your Ngrok Token!\")\n",
        "\n",
        "# Kill old tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Run\n",
        "subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "time.sleep(5)\n",
        "\n",
        "# Open Tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(8501).public_url\n",
        "    print(f\"DASHBOARD LIVE: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Tunnel Error: {e}\")"
      ],
      "metadata": {
        "id": "sLzSStgP6mu2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}